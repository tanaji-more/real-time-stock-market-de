Real-Time Stocks Market Data Pipeline
Snowflake DBT Apache Airflow Python Kafka Docker Power BI

ğŸ“Œ Project Overview
This project demonstrates an end-to-end real-time data pipeline using the Modern Data Stack.
We capture live stock market data from an external API, stream it in real time, orchestrate transformations, and deliver analytics-ready insights â€” all in one unified project.

Architecture (1)

âš¡ Tech Stack
Snowflake â†’ Cloud Data Warehouse
DBT â†’ SQL-based Transformations
Apache Airflow â†’ Workflow Orchestration
Apache Kafka â†’ Real-time Streaming
Python â†’ Data Fetching & API Integration
Docker â†’ Containerization
Power BI â†’ Data Visualization
âœ… Key Features
Fetching live stock market data (not simulated) from an API
Real-time streaming pipeline with Kafka
Orchestrated ETL workflow using Airflow
Transformations using DBT inside Snowflake
Scalable cloud warehouse powered by Snowflake
Analytics-ready Power BI dashboards
ğŸ“‚ Repository Structure
real-time-stocks-pipeline/
â”œâ”€â”€ producer/                     # Kafka producer (Finnhub API)
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ consumer/                     # Kafka consumer (MinIO sink)
â”‚   â””â”€â”€ consumer.py
â”œâ”€â”€ dbt_stocks/models/
â”‚   â”œâ”€â”€ bronze
â”‚   â”‚   â”œâ”€â”€ bronze_stg_stock_quotes.sql
â”‚   â”‚   â””â”€â”€ sources.yml
â”‚   â”œâ”€â”€ silver
â”‚   â”‚   â””â”€â”€ silver_clean_stock_quotes.sql
â”‚   â””â”€â”€ gold
â”‚       â”œâ”€â”€ gold_candlestick.sql
â”‚       â”œâ”€â”€ gold_kpi.sql
â”‚       â””â”€â”€ gold_treechart.sql
â”œâ”€â”€ dag/
â”‚   â””â”€â”€ minio_to_snowflake.py
â”œâ”€â”€ docker-compose.yml            # Kafka, Zookeeper, MinIO, Airflow, Postgres
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                     # Documentation
ğŸš€ Getting Started
Clone this repo and set up environment
Start Kafka + Airflow services via Docker
Run the Python producer to fetch live stock data
Data flows into Snowflake â†’ DBT applies transformations
Orchestrate everything with Airflow
Connect Power BI for visualization
âš™ï¸ Step-by-Step Implementation
1. Kafka Setup
Configured Apache Kafka locally using Docker.
Created a stocks-topic to handle live stock market events.
Defined producers (API fetch) and consumers (pipeline ingestion).
2. Live Market Data Producer
Developed Python producer script stock_producer.py to fetch real-time stock prices from the Finnhub API using an API key.
Streams stock data into Kafka in JSON format.
Producer Code
3. Kafka Consumer â†’ MinIO
Built Python consumer script stock_consumer.py to consume streaming data from Kafka.
Stored consumed data into MinIO buckets (S3-compatible storage).
Organized storage into folders for raw/bronze layer ingestion.
Consumer Code
4. Airflow Orchestration
Initialized Apache Airflow in Docker.
Created DAG (stock_pipeline_dag.py) to:
Load data from MinIO into Snowflake staging tables (Bronze).
Schedule automated runs every 1 minute.
Airflow DAGs
5. Snowflake Warehouse Setup
Created Snowflake database, schema, and warehouse.
Defined staging tables for Bronze â†’ Silver â†’ Gold layers.
SQL scripts available at:
Snowflake Setup
6. DBT Transformations
Configured DBT project with Snowflake connection.
Models include:
Bronze models â†’ raw structured data
Silver models â†’ cleaned, validated data
Gold models â†’ analytical views (Candlestick, KPI, Tree Map)
7. Power BI Dashboard
Connected Power BI to Snowflake (Gold layer) using Direct Query.
Built:
Candlestick chart â†’ stock market patterns
Tree chart â†’ stock price trends
gauge charts â†’ stock volume & total sales breakdown
KPI's â†’ real-time sortable view
ğŸ“Š Final Deliverables
Automated real-time data pipeline
Snowflake tables (Bronze â†’ Silver â†’ Gold)
Transformed analytics models with DBT
Orchestrated DAGs in Airflow
Power BI dashboard with live insights
